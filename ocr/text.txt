I'd like to begin this paper -- as I often seem to -- by revising my title, where I've used the word "vestigial" for functionally formal elements of a design that persist beyond their specific practicalities but survive aesthetic remainders. On reflection, I'm not sure that the biological, evolutionary metaphor is entirely appropriate. Nor am I convinced that skeuomorphism fully captures the phenoma of typeface functionally, since I understand that term to describe a backwards-looking process where obsolete function becomes a metaphor for new function. Instead, I think holomorphosis might better capture the phenomenon I wish to explore today, where an artifact of one outmoded process becomes emblematic of other, contemporaneous processes. 

In any case, I'm interested in talking today about a particular typeface or font, OCR-A, and I'd like to trace the significance of some of its design applications back to the context in which it originated. The occasion of this panel on a media archaoelogical approach to "the invisible and unwanted", provides two convenient frames from which to explore this significance.

First, invisibility is an ironically appropriate concept for discussing typography, since type operates within the particular sub-visibility of graphic design whereby it is often said to work best when it is noticed least. As Beatrice Warde states in her exquisitely Modernist essay, "The Crystal Goblet", "The book typographer has the job of erecting a window between the reader inside the room and that landscape which is the author's words." Even if OCR-A is a relatively "opaque" font, its uses outside of optical character recognition eclipse the specific circumstances constraining each of its letters.

A second useful frame for this panel, media archaeology is, as I understand it a way of doing media theory with a historical perspective toward the conditions and possibilities that make culture possible. For OCR-A, its conditions, possibilities and necessities are one answer to the question that began my interest in OCR-A. That is, why is it that OCR-A appears so frequently in paratextual design related to videogames, particularly if the apparent intent of the design is to create an association with older games? In fact, since OCR-A was intended to make alphanumeric characters printed in ink more reliably readable by data processing machines, it was never intended for screen display and is rarely, if at all, used in videogame packaging or arcade cabinet design until the 1990s. In other words, its retrogame association is anachronistic. Even so, the history of OCR-A's development and the way its design emblematizes that history reveals a patterning toward obsolescence characteristic of how we tell stories about the futures of technology's past.

The basic processes and applications for Optical Character Recognition technology are familiar to anyone who has worked in digitizing documents or who has searched for something in Google Books. Basically, a digital scan of a page image is examined by software that identifies specific alphabetic graphemes and converts them into some digital format such as ASCII. This is the workflow by which, for example, Google Books is converting "all of the worlds knowledge" into an indexical format, ready for searching, collecting and n-gramming.

This all generally works well enough that, from a readerly point of view, we tend only to notice it when it fails. This is the point at which its byproducts have the potential to become aesthetic artifacts. For example, reCaPTCHA is a service that combines OCR recognition with a human input element. It creates an authentication barrier asking the user to prove that she is not a machine while, at the same time, recruiting her human subjectivity read some word that the OCR software could not make out. What the user sees is two words: one, already known to the software, has been artificially distorted to baffle further machine translation; the other is just organically distorted by the vaguaries of the printing or scanning process. The juxtaposition of these two barely readable words can be provocative or amusing, enough to provide punchlines for webcomics and an entire "saga of inglip" that in treats recaptcha word pairs as commands issued from a mysterious, possibly lovecraftian horror. These comics and other manipulations like them exploit the oddness that lies at the threshold between machine readability and human subjectivity, positioning the latter as the Other of the former.

Extending a similarly liminal subjectivity, a tumblr blog by Krissy Wilson showcases "The Art of Google Books," appropriating the errata of Google's OCR with a new aesthetic sensibility, often showing the traces of the unnamed owners, donors, readers, or archivists of these books who preside over the twilight of their material specificity. Like the phenomenon of captcha art, this "adversaria" of the miscellany of Google's 20 million or so books is interesting because it only becomes evident when the otherwise smooth process of text encoding becomes too noisy. Error, or some other mitigations against the imaginary one-to-one conversion of text to data intervenes, and that error is almost always human.

Nowadays, the human computer relationship is largely a question of interfaces built on metaphors, but in the earliest implementations of mainframe computers, where the primary business tasks relegated to computing involved the processing or storing large sets of data, efficient and reliable data entry was an important early problem to solve. A 1970 handbook on OCR acknowledges that, "Each day it becomes more evident that our large and complex computers do no operate in a void but in a social environment where they communicate with human beings as well as with other data processing equipment." This book elaborates that within the data-processing domain, there is a human-computer problem where, of the two parties involved, culpability lies with the human who is slow and prone to error. OCR-A was the product of an attempt to solve that problem through standardization.

The earliest demonstrations of optical typographic character recognition date back to the late 19th century, and a 1914 patent for a "Photoprinting Apparatus" is the first practical demonstration of something like modern OCR in an industrial context. Large-scale data processing began to develop in scientific and military computing with the development of mainframe computing through the 1940s. But by the early 1960s, OCR advocates and inventors like Jacob Rabinow, who had consulted with Bush on the Memex, were calling for an industry standard to support the development of the technology as well as the emerging secondary industry in providing OCR solutions. Computers were being used to process, calculate, and track more and more information, and standardized OCR leveraged the durability of ink and paper to replace slower, more fragile systems like [Hollerith] punch cards.

In the United States, the solution to this problem, the typeface we now know as OCR-A, took shape in the mid-1960s, with implementations as early as 1966. The full type definition was not publicly available until the 1968 publication of x.3. [whatever] document, by the USA Standards Institute (which later became the more familiar ANSI), in accordance with ISO standard [whatever]. This document defines details of the font in minute detail, down to relative stroke widths and ink smudge tolerance, because its provenance is entirely technical. Throughout the document, though the human reader is implied, it is clear that the primary audience for text printed in this font is the OCR reading device. Thus, the shapes of each character become constrained and distorted by the affordances of that reading machines resolution and tolerance for error. Great care is taken, for example, to distingish between otherwise similar characters such as I, l and 1 or O, 0 and D. 

The result is a font that mixes serif with sans-serif characters, shifts axes and internal symmetry haphardzardly, and is easily recognizable through its acute angularity. In these ways, OCR-A is an ugly font, but that ugliness makes it considerable less "invisible" than, say, Helvetica, in a way that is directly tied to its technical function. Note how this inverts the situation recognized in The Art of Google Books where the human element creates the technical remainder. Here it is precisely the characteristics of OCR-A that are NOT intended for humans that lead to its aesthetic function.

With the question here of readership and readability coming to imply both human and machine subjectivities, it is worth noting, with some irony, that the first major business use of OCR-A was in 1966 at Reader's Digest, which used OCR-A in reading, processing and printing and reading address labels. [can I find a image of that?] OCR-A thus supported reader's digestion of literature by enabling the digestion of readers by computers.

Seeking a middle ground, type designer Adrian Frutiger received a commission by ECMA to create a viable alternative to OCR-A. Writing in a typography journal in 1967, Frutiger declared that OCR-A was "barely readable" and "offensive to human taste." His solution in the OCR-B font ([iso standard whatever]) is much gentler and internally consistent, mainly owing to its use of curved lines. Like OCR-A, OCR-B is still attuned the felicities of machine resolution and the affordances of smudging and other vaguaries, but its design documentation and advocacy make the strong case, often in somewhat strident tones, that human readership is a much higher priority than is the case for OCR-A.

Perhaps it is this stridency on behalf of OCR-B that led Jacob Rabinow, writing for the industry journal _Datamation_ in 1969, to speak out in defence of the A standard. First noting that, so far as he knows, no one had yet been injured by OCR-A, Rabinow proceeds to dismantle several of Frutigers claim. Interestingly for a trade journal, Rabinow brackets a more technical description of OCR-B's shortcomings and instead calls into question the supposed aesthetic superiority of OCR-B.

[longer quote]

[graphlex camera image]

[lartigue image]

[bus image]

As technical functions become vestiges for aesthetic paradigms for industrial design, what we see here in 1969 as a defense of OCR-A is an articulation of the specific design patterns William Gibson would later explore in "The Gernsback Continuum" as [did he give it this name?] the Raygun Gothic. This futurism for a future that didn't or hasn't yet happened.

Already by the late 1960s, OCR-A had joined other systems like barcodes and Magnetic Ink Character Recognition (MICR) as part of the informational layer on day to reality, particular in retail environments. But unlike the ubiquitous barcodes and the MICR font e-13b that still appears on paper checks, OCR-A as a specific implementation drifted much more readily out of the functional domain and into aesthetics, as scanning hardware improved image resolution and software has improved beyond the necessity of such a stylized alphabet. Though there are still applicatons for OCR-A, one is much more likely to encounter it in graphic design, where -- owing to its angularity -- OCR-A is typically classified as a font for titles and display, as opposed to body text.

Here are just a few examples where OCR-A appears in book titles. Any apparent thematic continuity among these titles could owe something to the coincidence of being on my bookshelves -- it's difficult, after all, to query a database of books for which typefaces are used in cover art -- or, more likely, it could be that there is a recurring association created by these paratexts.

In a more striking illustration of thematic continuity between text and paratext vis a vis typeface occurs in a series of book covers by [penguin? whoever?] for William Gibson's novels and short story collections. Gibson's fiction expresses a range of ideas dealing with personal memory, the durability of storage media, artifical subectivies, and the obsolescence of futuristic product industrial design, all of which could also describe the specific cultural, economic, and commercial situations which created the need that OCR-A met in the 1960s.  

The 1999 film, The Matrix, OCR-A plays a role in a significant plot point. It's presence could have something to do with the obvious influence of Gibson's cyberpunk on the look of the film, or it could be something more specific. In the first act, Agent Smith sits across a table to interrogate Neo. This is the first scene when viewers become convinced that something is well and truly not right with the world, and as a possible hint toward what that might be, the file Agent Smith reads from is printed entirely in OCR-A. That is, a font created to be read by machine eyes is here being perused by a machine masquerading as a human inside of a simulation.

Each of these applicatons of OCR-A, from the functional to the aesthetic, accumulates associations and resonances that tie together the cultural, material, and economic conditions that brought it into being as well as those that justify its inclusion as a thematic design element. The relationship between videogames and OCR-A reflects this as well. 

One final bit of evidence ties this up nicey in the signage produced for the Art of Videogames Exhibit recently at the Smithsonian American Art Museum. Here, in a gallery full of pristine game consoles displayed alongside video of their respective library of games, each piece of signage or information text is printed in OCR-A. OCR-A actually predates any console on display at the Smithsonian by at least 10 years, placing it squarely in the era of those consoles' forerunners like Spacewar and the Brown Box. Here, the associative logic of the font's aesthetic remainder comes full circle as it sits alongside game console hardware that has been practically fetishized in a way that gives it cultural life beyond its obsolescence. Another example of how the vestiges of obsolescence shift into the aesthetic domain as they become the means by which we tell stories about the past.





